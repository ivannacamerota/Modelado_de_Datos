{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://media.geeksforgeeks.org/wp-content/uploads/Dimensionality_Reduction_2.jpg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://media.geeksforgeeks.org/wp-content/uploads/Dimensionality_Reduction_2.jpg</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color= #2E9AFE> Tema: Reducción de dimensionalidad</font>\n",
    "\n",
    "La selección de características es diferente a la reducción de dimensionalidad. Ambos métodos buscan reducir la cantidad de atributos en el conjunto de datos, pero un método de reducción de dimensionalidad lo hace creando nuevas combinaciones de atributos, donde los métodos de selección de características incluyen y excluyen atributos presentes en los datos sin cambiarlos.\n",
    "\n",
    "Ejemplos de métodos de reducción de dimensionalidad incluyen: Análisis de componentes principales y Descomposición de valores singulares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principant Component Analysis \n",
    "\n",
    "El PCA usa álgebra lineal para transformar el conjunto de datos en una forma comprimida.\n",
    "\n",
    "Generalmente, se denomina como técnica de reducción de datos. Una propiedad es que puede elegir el número de dimensiones o componentes principales en el resultado transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerías\n",
    "import pandas as pd \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "wine_data = datasets.load_wine(as_frame=True, return_X_y=False)\n",
    "df = wine_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "df['label'] = wine_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    x='label', \n",
    "    data=df)\n",
    "plt.title('Wine value count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separar X y Y\n",
    "X = wine_data.data\n",
    "y = wine_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escalar datos \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_scaled = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligiendo el número de componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "#inicializar el objeto\n",
    "pca = PCA()\n",
    "#aplicarlo a los datos\n",
    "pca_fit = pca.fit_transform(x_scaled)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la varianza acumulativa de cada componente\n",
    "plt.figure(figsize = (15, 6))\n",
    "components = np.arange(1, 14, step=1)\n",
    "variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(components, variance, marker='o', linestyle='--', color='green')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 14, step=1))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "plt.axhline(y=0.90, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '90% variance threshold', color = 'red', fontsize=16)\n",
    "plt.text(25, 0.85, \"Components needed: \"+str(np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9)[0][0]), color = \"red\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico anterior contiene la varianza acumulada como una proporción en el eje Y y el número de componentes en el eje X.\n",
    "\n",
    "Utilizando un umbral de variación del 90%, el gráfico anterior nos ayuda a determinar cuántos componentes debemos conservar de nuestro conjunto de datos para que siga teniendo sentido para nosotros en cualquier modelado posterior.\n",
    "\n",
    "Tenga en cuenta que aquí elegimos el 90% como umbral de variación, pero esta no es una regla de oro. El data scientist elige este umbral de variación.\n",
    "\n",
    "Entonces, podemos ver en este gráfico que necesitamos 7 componentes para retener el 90% de la variabilidad (información) en nuestros datos. Eso no es una gran mejora con respecto al número total de variables, 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "components = np.arange(1, 14, step=1)\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.plot(components, eigenvalues, marker = 'o', \n",
    "                 linestyle = '--', color = 'green')\n",
    "plt.ylim(0, max(eigenvalues))\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 14, step = 1))\n",
    "plt.title('Scree plot')\n",
    "plt.axhline(y=1, color = 'r', linestyle = '-')\n",
    "plt.text(0, 0.75, 'Eigenvalue Cutoff', color = 'red', fontsize=14)\n",
    "plt.text(15, 1.10, 'Components Needed: '+str(np.where(eigenvalues<=1)[0][0]), \n",
    "                 color = 'red', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El anterior contiene los eigenvalores en el eje Y y el número de componentes en el eje X.\n",
    "\n",
    "Como regla general, debemos seleccionar la cantidad de componentes que tienen un valor propio mayor que 1. Según el diagrama anterior, debemos optar por mantener 3 componentes. Esto es muy diferente al gráfico de varianza acumulada anterior donde elegimos retener 7 componentes.\n",
    "\n",
    "Sin embargo, si optamos por conservar 3 componentes, esto sólo explica el 65% de la variabilidad. Esto es mucho menor que la variabilidad capturada al retener 7 componentes (90%). En este punto, científico de datos tendría que decidir si una variación acumulada del 65 % es satisfactoria en función de su conocimiento del dominio y otras investigaciones en el área."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que:\n",
    "\n",
    "- El primer componente principal explica 36% del total de la varianza de los datos.\n",
    "- El segundo componente principal explica 19% del total de la varianza de los datos.\n",
    "- El tercer componente principal explica 11% del total de la varianza de los datos.\n",
    "- El cuarto componente principal explica 7% del total de la varianza de los datos.\n",
    "- Etc.\n",
    "\n",
    "Nota: los porcentajes suman 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando el PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "#inicializar el objeto\n",
    "pca = PCA(n_components=3)\n",
    "#aplicarlo a los datos\n",
    "pca_fit = pca.fit_transform(x_scaled)\n",
    "datos_pca = pd.DataFrame(pca_fit, columns=['PC1','PC2','PC3'])\n",
    "datos_pca['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sin PCA\n",
    "##Aplicacion de la regresion logística con datos originales\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#inicializar el objeto\n",
    "logreg = LogisticRegression()\n",
    "#aplicar la regresión logística a los datos\n",
    "logreg.fit(x_scaled, y)\n",
    "\n",
    "#predicciones\n",
    "y_predict = logreg.predict(x_scaled)\n",
    "accuracy_score(y,y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Aplicacion de la regresion logística con datos después de aplicar el PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(np.array(datos_pca[['PC1','PC2','PC3']]), y)\n",
    "\n",
    "y_predict = logreg.predict(datos_pca[['PC1','PC2','PC3']])\n",
    "accuracy_score(y,y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se aumenta el accuracy con el PCA (fenómeno normal) pero sí reducimos la dimensionalidad de los datos, por lo tanto es menos carga computacional. Con menos información, el clasificador sigue dando muy buenos resultados. \n",
    "\n",
    "Si son millones de datos y pocas variables, sí conviene hacer el PCA. También conviene en el caso de tener muchas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué algoritmos se benefician del PCA?\n",
    "\n",
    "1. Algoritmos lineales. Ya que ayuda a reducir la multicolinealidad\n",
    "    - Regresión logística\n",
    "    - Regresión Lineal\n",
    "2. Máquinas de vector soporte\n",
    "3. Algoritmos de clustering\n",
    "4. Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más información sobre el PCA:\n",
    "\n",
    "https://towardsdatascience.com/the-most-gentle-introduction-to-principal-component-analysis-9ffae371e93b#:~:text=From%20Wikipedia%2C%20PCA%20is%20a,find%20unmeasured%20%E2%80%9Clatent%20variables%E2%80%9D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Sara E. Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
