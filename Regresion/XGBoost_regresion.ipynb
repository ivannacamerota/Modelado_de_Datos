{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/1400/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://miro.medium.com/max/1400/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: XGBoost - Regresión</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros\n",
    "\n",
    "1. **eta (learning_rate)**: rango: [0,1] Es un parámetro que evita el sobreajuste.\n",
    "2. **min_child_weight** : rango: [0,∞] En la tarea de regresión, esto corresponde al número mínimo de observaciones necesarias para cada nodo. Se utiliza para controlar el sobreajuste.\n",
    "3. **max_depth**: rango: [0,∞] Profundidad máxima de un árbol. Aumentar este valor hará que el modelo sea más complejo y sea más probable que se sobreajuste. 0 indica que no hay límite de profundidad. Ten en cuenta que XGBoost consume memoria de forma agresiva al entrenar un árbol muy profundo. Se suele recomendar tomar valores entre 3 y 10.\n",
    "4. **gamma**: rango: [0,∞] método para podar \"prune\" el árbol\n",
    "5. **lambda**: Dado que el valor lambda está en el denominador de la similitud, a medida que la lambda aumenta, la similitud disminuirá y, por lo tanto, esto también disminuirá la ganancia. Esto permite una mayor poda, solo se conservan y sobreajustan las ramas con una puntuación de ganancia alta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Los datos los podemos obtener de: https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\n",
    "\n",
    "El conjunto de datos contiene 9568 datos recopilados de una central eléctrica durante 6 años (2006-2011), cuando la central eléctrica estaba configurada para funcionar a plena carga.\n",
    "\n",
    "Una central eléctrica de ciclo combinado (CCPP) está compuesta por turbinas de gas, turbinas de vapor y generadores de vapor con recuperación de calor. En una CCPP, la electricidad se genera mediante turbinas de gas y de vapor, que se combinan en un ciclo, y se transfiere de una turbina a otra. \n",
    "\n",
    "\n",
    "Columnas disponibles (promedio por hora):\n",
    "- AT: Temperatura\n",
    "- V: Vacío de Escape\n",
    "- AP: Presión Ambiental\n",
    "- RH: Humedad Relativa\n",
    "- PE: Producción neta de energía eléctrica.\n",
    "\n",
    "Power output (energía) es la variable a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/ab/a2/876d56ae72d7472b7a4228b880f1aaaa9c01817e05b4943674c9384ff20a/xgboost-2.1.2-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata\n",
      "  Downloading xgboost-2.1.2-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/ivo/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/ivo/anaconda3/lib/python3.11/site-packages (from xgboost) (1.10.1)\n",
      "Downloading xgboost-2.1.2-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m788.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3905289949.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    conda install -c anaconda py-xgboost\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install xgboost\n",
    "# conda install conda-forge::xgboost\n",
    "conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar datos\n",
    "datos = pd.read_excel(\"Folds5x2_pp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vistazo de los datos\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tamaño de los datos\n",
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tipo de datos\n",
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores nulos?\n",
    "datos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre variable de salida y la temperatura\n",
    "datos.plot(x ='AT', y = 'PE', kind =\"scatter\", \n",
    "                 figsize = [10,10],\n",
    "                 color =\"b\", alpha = 0.3)\n",
    "plt.title(\"Temperatura vs Energía\")\n",
    "plt.xlabel(\"Temperatura\") \n",
    "plt.ylabel(\"Energía\")\n",
    "plt.show()\n",
    "\n",
    "#Existe una correlación *negativa* entre la temperatura y la energía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre variable de salida y el Exhaust Vacuum Speed\n",
    "datos.plot(x ='V', y = 'PE', kind =\"scatter\", \n",
    "                 figsize = [10,10],\n",
    "                 color =\"b\", alpha = 0.3)\n",
    "plt.title(\"Exhaust Vacuum Speed vs Energía\")\n",
    "plt.xlabel(\"Exhaust Vacuum Speed\") \n",
    "plt.ylabel(\"Energía\")\n",
    "plt.show()\n",
    "\n",
    "#Existe una correlación *negativa* entre la energia y el Exhaust Vacuum Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre variable de salida y la Presión Ambiental\n",
    "datos.plot(x ='AP', y = 'PE', kind =\"scatter\", \n",
    "                 figsize = [10,10],\n",
    "                 color =\"b\", alpha = 0.3)\n",
    "plt.title(\"Presión Ambiental vs Energía\")\n",
    "plt.xlabel(\"Presión Ambiental\") \n",
    "plt.ylabel(\"Energía\")\n",
    "plt.show()\n",
    "\n",
    "#Existe una correlación *positiva* débil entre la energía y la presión ambiental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre variable de salida y la Presión Ambiental\n",
    "datos.plot(x ='RH', y = 'PE', kind =\"scatter\", \n",
    "                 figsize = [10,10],\n",
    "                 color =\"b\", alpha = 0.3)\n",
    "plt.title(\"Humedad Relativa vs Energía\")\n",
    "plt.xlabel(\"Humedad Relativa\") \n",
    "plt.ylabel(\"Energía\")\n",
    "plt.show()\n",
    "\n",
    "#Existe una correlación *positiva* débil entre la energía y la humedad relativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráfico de correlacion\n",
    "corr = datos.corr()\n",
    "plt.figure(figsize = (9, 7))\n",
    "sns.heatmap(corr, cmap=\"RdBu\",\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar nuestras X de las Y\n",
    "X = datos.drop(\"PE\", axis = 1).values\n",
    "y = datos['PE'].values\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "#Dividiendo los datos en prueba y entrenamiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                               test_size = 0.2, \n",
    "                                               random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construir el xgboost\n",
    "#inicializar objeto de regresión\n",
    "modelo = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
    "\n",
    "#Entrenar modelo\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "#predicciones\n",
    "y_hat= modelo.predict(X_test)\n",
    "\n",
    "#medidas de performance\n",
    "r2 = metrics.r2_score(y_test,y_hat)\n",
    "mse = metrics.mean_squared_error(y_test,y_hat)\n",
    "print('R2:',r2)\n",
    "print(\"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quieremos optimizar los hiperparámetros del modelo podemos utilizar grid search  + crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos la profundidad vs la R2\n",
    "max_depths = range(1, 21)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for max_depth in max_depths:   \n",
    "    tree_reg = xgb.XGBRegressor(max_depth=max_depth, objective='reg:squarederror', seed=42)\n",
    "    tree_reg.fit(X_train, y_train)\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(max_depths, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(max_depths, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('Tree Max Depth')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto de la profundidad del árbol en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos la eta (learning_rate) vs la R2\n",
    "learning_rates = [0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for learning_rate in learning_rates:   \n",
    "    tree_reg = xgb.XGBRegressor(learning_rate=learning_rate, objective='reg:squarederror', seed=42)\n",
    "    tree_reg.fit(X_train, y_train)\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(learning_rates, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(learning_rates, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('learning_rates')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto de la tasa de aprendizaje en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos la gamma vs la R2\n",
    "gammas = range(1, 21)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for gamma in gammas:   \n",
    "    tree_reg = xgb.XGBRegressor(gamma=gamma, objective='reg:squarederror', seed=42)\n",
    "    tree_reg.fit(X_train, y_train)\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(gammas, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(gammas, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('gammas')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto de la gamma en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos la lambda vs la R2\n",
    "lambdas = range(1, 10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for l in lambdas:\n",
    "    tree_reg = xgb.XGBRegressor(reg_lambda=l, objective='reg:squarederror', seed=42)\n",
    "    tree_reg.fit(X_train, y_train)\n",
    "    train_scores.append(tree_reg.score(X_train, y_train))\n",
    "    test_scores.append(tree_reg.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lambdas, train_scores, label='Train $R^2$', color='blue', marker='o')\n",
    "plt.plot(lambdas, test_scores, label='Test $R^2$', color='green', marker='o')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('Efecto del las lambdas en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parámetros para el grid search\n",
    "gbm_param_grid = {\n",
    "     'reg_lambda': [5,6,7] , \n",
    "     'gamma': [8,9,10,11,12],\n",
    "     'learning_rate': [0.05, 0.1, 0.2],\n",
    "     'max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "#Iniciar el modelo\n",
    "gbm = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
    "\n",
    "#Hacer el grid search\n",
    "grid_mse = GridSearchCV(estimator = gbm, param_grid = gbm_param_grid, scoring = 'neg_mean_squared_error', cv = 2, verbose = 1)\n",
    "\n",
    "#Ajustar a los datos de entrenamiento\n",
    "grid_mse.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejores hiperparámetros encontrados: \",grid_mse.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construir y evaluar el XGBoost con los hiperparámetros óptimos\n",
    "modelo_nuevo = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                           seed=42,\n",
    "                           max_depth=5,\n",
    "                           gamma=9,\n",
    "                           learning_rate = 0.2,\n",
    "                           reg_lambda=5)\n",
    "\n",
    "#Entrenar modelo\n",
    "modelo_nuevo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predecir\n",
    "y_hat= modelo_nuevo.predict(X_test)\n",
    "\n",
    "#metricas de performance\n",
    "r2 = metrics.r2_score(y_test,y_hat)\n",
    "mse = metrics.mean_squared_error(y_test,y_hat)\n",
    "print('R2:',r2)\n",
    "print(\"MSE:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas**\n",
    "- Modelo muy efectivo con bases de datos grandes y complejas\n",
    "- No necesita escalamiento de variables\n",
    "- Funciona bien con datos no lineales\n",
    "- Modelo que muestra la importancia de las variables\n",
    "- Tiene una herramienta interna para trabajar con datos nulos\n",
    "\n",
    "**Desventajas**\n",
    "- Modelo de Caja negra -> no se puede interpretar\n",
    "- Es más difícil tunear los hiperparámetros\n",
    "- Modelo sensible a datos atípicos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
